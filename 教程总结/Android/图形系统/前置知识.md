https://juejin.cn/post/6863756420380196877#heading-1
//todo 硬件绘制 https://juejin.cn/post/6844903652847517710

性能指标
OpenGLRenderer: Davey! duration
Choreographer: Skipped

显示系统基础知识
显示系统一般包括 CPU, GPU 以及 Display 三个部分，CPU负责计算帧数据，把计算好的数据交给GPU，GPU会对图形数据进行渲染，
渲染好后放到buffer(图像缓冲区)里存起来，然后Display（屏幕或显示器）负责把buffer里的数据呈现到屏幕上。
android_渲染机制_screen与缓冲区.png

当然实际上的流程要复杂许多，比如说 APP 绘制过程中会涉及到软件绘制和硬件加速，以及通过 Surface 向图形缓存区写入渲染数据，
另外在 SurfaceFlinger 进程会将这些不同 Surface(Layer) 的数据合成交给显示器的缓存区等等


基础概念
屏幕刷新频率
一秒内屏幕刷新的次数（一秒内显示了多少帧的图像），单位 Hz（赫兹），如常见的 60 Hz。刷新频率取决于硬件的固定参数（不会变的）。

逐行扫描
显示器并不是一次性将画面显示到屏幕上，而是从左到右边，从上到下逐行扫描，顺序显示整屏的一个个像素点，不过这一过程快到人眼无法察觉到变化。
以 60 Hz 刷新率的屏幕为例，这一过程即 1000 / 60 ≈ 16ms。

帧率 （Frame Rate）
表示 GPU 在一秒内绘制操作的帧数，单位 fps。例如在电影界采用 24 帧的速度足够使画面运行的非常流畅。而 Android 系统则采用更加流程的 60 fps，
即每秒钟GPU最多绘制 60 帧画面。帧率是动态变化的，例如当画面静止时，GPU 是没有绘制操作的，屏幕刷新的还是buffer中的数据，
即GPU最后操作的帧数据。

画面撕裂（tearing）
一个屏幕内的数据来自2个不同的帧，画面会出现撕裂感，如下图
android_渲染机制_画面撕裂.jpg


双缓存
画面撕裂 原因
屏幕刷新频是固定的，比如每16.6ms从buffer取数据显示完一帧，理想情况下帧率和刷新频率保持一致，即每绘制完成一帧，显示器显示一帧。
但是CPU/GPU写数据是不可控的，所以会出现buffer里有些数据根本没显示出来就被重写了，即buffer里的数据可能是来自不同的帧的，
当屏幕刷新时，此时它并不知道buffer的状态，因此从buffer抓取的帧并不是完整的一帧画面，即出现画面撕裂。
简单说就是Display在显示的过程中，buffer内数据被CPU/GPU修改，导致画面撕裂。

双缓存
那咋解决画面撕裂呢？ 答案是使用 双缓存。
由于图像绘制和屏幕读取 使用的是同个buffer，所以屏幕刷新时可能读取到的是不完整的一帧画面。
双缓存，让绘制和显示器拥有各自的buffer：GPU 始终将完成的一帧图像数据写入到 Back Buffer，而显示器使用 Frame Buffer，
当屏幕刷新时，Frame Buffer 并不会发生变化，当Back buffer准备就绪后，它们才进行交换。如下图：
android_渲染机制_双缓存.png


双缓存与VSync
问题又来了：什么时候进行两个buffer的交换呢？
假如是 Back buffer准备完成一帧数据以后就进行，那么如果此时屏幕还没有完整显示上一帧内容的话，肯定是会出问题的。
  看来只能是等到屏幕处理完一帧数据后，才可以执行这一操作了。
当扫描完一个屏幕后，设备需要重新回到第一行以进入下一次的循环，此时有一段时间空隙，称为VerticalBlanking Interval(VBI)。
那，这个时间点就是我们进行缓冲区交换的最佳时间。因为此时屏幕没有在刷新，也就避免了交换过程中出现 screen tearing的状况

VSync(垂直同步)是VerticalSynchronization的简写，它利用VBI时期出现的vertical sync pulse（垂直同步脉冲）来保证双缓冲在最佳时间点才进行交换。
  另外，交换是指各自的内存地址，可以认为该操作是瞬间完成。

所以说V-sync这个概念并不是Google首创的，它在早年的PC机领域就已经出现了。




Android屏幕刷新机制
Android4.1之前的问题
具体到Android中，在Android4.1之前，屏幕刷新也遵循 上面介绍的 双缓存+VSync 机制。如下图：
android_渲染机制_android4_1之前的双缓存.png

以时间的顺序来看下将会发生的过程：
1 Display显示第0帧数据，此时CPU和GPU渲染第1帧画面，且在Display显示下一帧前完成
2 因为渲染及时，Display在第0帧显示完成后，也就是第1个VSync后，缓存进行交换，然后正常显示第1帧
3 接着第2帧开始处理，是直到第2个VSync快来前才开始处理的。
4 第2个VSync来时，由于第2帧数据还没有准备就绪，缓存没有交换，显示的还是第1帧。这种情况被Android开发组命名为“Jank”，即发生了丢帧。  
   //这是关键第2帧的处理过晚并且不及时
5 当第2帧数据准备完成后，它并不会马上被显示，而是要等待下一个VSync 进行缓存交换再显示。
所以总的来说，就是屏幕平白无故地多显示了一次第1帧。

原因是 第2帧的CPU/GPU计算 没能在VSync信号到来前完成 。
我们知道，双缓存的交换 是在Vsyn到来时进行，交换后屏幕会取Frame buffer内的新数据，而实际 此时的Back buffer 就可以供GPU准备下一帧数据了。 
如果 Vsyn到来时  CPU/GPU就开始操作的话，是有完整的16.6ms的，这样应该会基本避免jank的出现了（除非CPU/GPU计算超过了16.6ms）。  
那如何让 CPU/GPU计算在 Vsyn到来时进行呢？



drawing with VSync
为了优化显示性能，Google在Android 4.1系统中对Android Display系统进行了重构，实现了Project Butter（黄油工程）：
系统在收到VSync pulse后，将马上开始下一帧的渲染。即一旦收到VSync通知（16ms触发一次），CPU和GPU 立刻开始计算然后把数据写入buffer。
如下图：
android_渲染机制_drawingWithVSync.png

CPU/GPU根据VSYNC信号同步处理数据，可以让CPU/GPU有完整的16ms时间来处理数据，减少了jank。
一句话总结，VSync同步使得CPU/GPU充分利用了16.6ms时间，减少jank。

问题又来了，如果界面比较复杂，CPU/GPU的处理时间较长 超过了16.6ms呢？如下图：
android_渲染机制_双缓存中处理超时.png
1 在第二个时间段内，但却因 GPU 还在处理 B 帧，缓存没能交换，导致 A 帧被重复显示。
2 而B完成后，又因为缺乏VSync pulse信号，它只能等待下一个signal的来临。于是在这一过程中，有一大段时间是被浪费的。
3 当下一个VSync出现时，CPU/GPU马上执行操作（A帧），且缓存交换，相应的显示屏对应的就是B。这时看起来就是正常的。
  只不过由于执行时间仍然超过16ms，导致下一次应该执行的缓冲区交换又被推迟了——如此循环反复，便出现了越来越多的“Jank”

为什么 CPU 不能在第二个 16ms 处理绘制工作呢？
原因是只有两个 buffer，Back buffer正在被GPU用来处理B帧的数据， Frame buffer的内容用于Display的显示，这样两个buffer都被占用，
CPU 则无法准备下一帧的数据。 那么，如果再提供一个buffer，CPU、GPU 和显示设备都能使用各自的buffer工作，互不影响。


三缓存
三缓存就是在双缓冲机制基础上增加了一个 Graphic Buffer 缓冲区，这样可以最大限度的利用空闲时间，
  带来的坏处是多使用的一个 Graphic Buffer 所占用的内存。
android_渲染机制_三缓存.png
1 第一个Jank，是不可避免的。但是在第二个 16ms 时间段，CPU/GPU 使用 第三个 Buffer 完成C帧的计算，虽然还是会多显示一次 A 帧，
  但后续显示就比较顺畅了，有效避免 Jank 的进一步加剧。
2 注意在第3段中，A帧的计算已完成，但是在第4个vsync来的时候才显示，如果是双缓冲，那在第三个vynsc就可以显示了。
   //由于有三个缓存，A的缓存先不管，Vsync到来，C的缓存交换找到前缓冲，A交换到后缓冲，A的缓存可以处理B了

三缓冲有效利用了等待vysnc的时间，减少了jank，但是带来了延迟。 所以，是不是 Buffer 越多越好呢？这个是否定的，Buffer 正常还是两个，
  当出现 Jank 后三个足以。
Triple Buffer：当双Buffer不够使用时，该系统可分配第三块Buffer    //todo jank时的三缓冲在哪
这就是Android渲染机制的发展


https://juejin.cn/post/6898525503960186887#heading-3
Android图形组件
开发者可通过三种方式将图像绘制到屏幕上：Canvas, OpenGL ES 或 Vulkan。无论使用什么渲染API，一切内容都会渲染到Surface，
Surface 表示 BufferQueue 中的生产方，而 BufferQueue 通常被 SurfaceFlinger 消费。
在 Android 平台上创建的每个 Window 都由 Surface 提供支持，所有被渲染的可见 Surface 都被 SurfaceFlinger 合成到显示部分。
下图显示了关键组件如何协同工作：
android_渲染机制_关键组件协同.awebp
相关组件如下：
Image Stream Producers: 图像流生产方可以是生成图形缓冲区以供消费的任何内容，例如 OpenGL ES、Canvas 2D 和 mediaserver 视频解码器。
Image Stream Consumers: 图像流最常见的消费者是 SurfaceFlinger，该系统服务会消费当前可见的 Surface，
    并使用 WindowManager 中提供的信息将它们合成交到 Display。SurfaceFlinger 使用 OpenGL 和 HardWare Composer 来合成 Surface。
   其他 OpenGL ES 应用也可以消费图像流，例如相机应用会消费相机预览图像流；非 GL 应用也可以是使用方，例如 ImageReader 类。
Hardware Composer: 这是显示子系统的硬件抽象层，SurfaceFlinger 可以将某些合成工作委托给 Hardware Composer，
   以分担 OpenGL 和 GPU 上的工作量。SurfaceFlinger 在收集可见层的所有缓冲区之后会询问 Hardware Composer 应如何进行合成。
Gralloc: 使用图形内存分配器 (Gralloc) 来分配图像生产方请求的内存。

BufferQueue
Android 图形数据流管道如下图：       对于BufferQueue，生产者一般是Surface，消费者一般是SurfaceFlinger
android_渲染机制_图形数据流管道.awebp
左侧的对象是生成图形缓冲区的渲染器，如主屏幕、状态栏和系统界面。SurfaceFlinger 是合成器，而 Hardware Composer 是制作器。
BufferQueue 是 Android 图形系统很重要的组成部分，它负责数据的传递：
android_渲染机制_BufferQueue的数据流动.awebp
图中的 producer 和 consumer 运行在不同的进程里，BufferQueue 是将缓冲区池与队列相结合的数据结构，它使用 Binder IPC 在进程之间传递缓冲区。
几个重要函数如下： 
producers 通过 BufferQueue 请求一块空闲的缓存区(GraphicBuffer): IGraphicBufferProducer.dequeueBuffer 方法
往缓存区(GraphicBuffer)填充了数据(绘制等)后, producers 将缓存区(GraphicBuffer)入队列到 BufferQueue 中:
    IGraphicBufferProducer.queueBuffer 方法
consumer 从 BufferQueue 中出队列一块缓存区(GraphicBuffer): IGraphicBufferConsumer.acquireBuffer 方法
consumer 消费完毕后(典型的是 SurfaceFlinger 合成数据)将缓存区(GraphicBuffer)返回给队列: IGraphicBufferConsumer.releaseBuffer 方法

其中 IGraphicBufferProducer 是 BufferQueue 的生产者接口，实现类是 BufferQueueProducer 生产者类；
   IGraphicBufferConsumer 是 BufferQueue 的消费者接口，实现类是 BufferQueueConsumer 消费者类。

frameworks/native/libs/gui/include/gui/BufferSlot.h
GraphicBuffer 有几种状态：
FREE: 该 Buffer 没有被 producer/consumer 所使用，其所有权属于 BufferQueue
DEQUEUED: 该 Buffer 被 producer 获取了，其所有权属于 producer
QUEUED: 该 Buffer 被 producer 填充了数据且入队列到 BufferQueue 了，其所有权属于 BufferQueue
ACQUIRED: 该 Buffer 被 consumer 获取了，该Buffer的所有权属于 consumer
SHARED: 该 Buffer 处于 shared buffer 模式

之所以需要这些状态，应该是为了维护一个 buffer pool 的结构，而不是每次使用 Buffer 时便创建一段共享内存，使用完毕便释放之，效率较低。
GraphicBuffer 状态的变化过程: FREE -> dequeueBuffer() -> DEQUEUED -> queueBuffer() -> QUEUED -> acquireBuffer() -> 
   ACQUIRED -> releaseBuffer() -> FREE


相关概念
View: 视图，绘制到屏幕上的内容，如 TextView, ImageView 等。
Window: View 的载体，对 Window 进行添加和删除需要通过 WindowManager 来操作。Window 并不是真实存在的，
     View 才是 Android 中的视图呈现形式，View 不能单独存在，它必须依附在 Window 这个抽象的概念上面。
WindowManager: 管理系统中的 Window, 实际功能通过 Binder IPC 借由 WindowManagerService 实现。
Canvas: 提供一些对 Surface 绘图的 API 用来进行实际的绘图操作。如果是软件绘制，其 drawXXX 方法会将内容绘制到 Bitmap 上；
    如果是硬件绘制，其 drawXXX 方法会抽象成 DrawOp 操作，然后添加到 DisplayList 中被 GPU 渲染。
Surface: 一个 Window 对应一个 Surface(当存在 SurfaceView 则例外，Java Surface 实例存在于 ViewRootImpl 中，对应 native 层的 Surface 对象)。
    Surface 内部持有一个 BufferQueueProducer 指针(在 Layer 中创建)可以生产图像缓存区用来绘图，
    与 App 和 SurfaceFlinger 形成一个生产者消费者模型。
Layer: App 请求创建 Surface 时 SurfaceFlinger 会创建 Layer 对象，它是 SurfaceFlinger 合成的基本操作单元，
    因此一个 Surface 对应一个 Layer。它创建有一个 BufferQueueProducer 生产者和 BufferQueueConsumer 消费者，
   这两个对象与像素数据的存储与转移相关。用户最终看到的屏幕内容是许多 Layer 按照 z-order 混合的结果。
SurfaceView: 一种较之 TextView, Button 等更为特殊的 View, 它不与其宿主的 Window 共享一个 Surface, 而是有自己的独立 Surface。
    并且它可以在一个独立的线程中绘制 UI。因此 SurfaceView 一般用来实现比较复杂的图像或动画/视频的显示。
Choreographer: 编舞者，用来控制当收到 VSync 信号后才开始绘制任务，保证绘制拥有完整的16.6ms。通常应用层不会直接使用Choreographer，
    而是使用更高级的API，如View.invalidate()等，可以通过Choreographer来监控应用的帧率。
    Google在Android 4.1系统中对Android Display系统进行了优化：在收到VSync pulse后，将马上开始下一帧的渲染。即一旦收到VSync通知，
    CPU和GPU就立刻开始计算然后把数据写入buffer。"drawing with VSync" 的实现——Choreographer。
    Choreographer，意为 舞蹈编导、编舞者。在这里就是指 对CPU/GPU绘制的指导—— 收到VSync信号 才开始绘制，保证绘制拥有完整的16.6ms，
    避免绘制的随机性。
SurfaceFlinger: 管理消费当前可见的 Surface，所有被渲染的可见 Surface 都被 SurfaceFlinger，
  通过 WindowManager 提供的信息合成(使用 OpenGL 和 HardWare Composer，合成的数据源是上面提及的 BufferQueue 中的 GraphicBuffer)提交到屏幕的后缓冲区，
  等待屏幕的下一个Vsync信号到来，再显示到屏幕上。SurfaceFlinger 通过屏幕后缓冲区与屏幕建立联系，同时通过 Surface 与上层建立联系，
  起到了一个承上启下的作用。
HWComposer: HardWare Composer, 定义一套 HAL 层接口，芯片厂商根据硬件特点来实现这套接口。
   其主要工作是将 SurfaceFlinger 计算后的 Layer 显示参数合成到显示器 Buffer 上。当然 SurfaceFlinger 并非是 HWC 的唯一输入源，
   例如摄像头的预览输入 Buffer 可以由硬件设备直接写入，然后作为 HWC 的输入之一与 SurfaceFlinger 的输出做最后的合成。
OpenGL: 一个 2D/3D 的图形库，需要底层硬件(GPU)和驱动来支持。移动端通常使用其子集 OpenGl ES(OpenGl for Embedded System)。
Display: 显示设备的抽象，传统的 Display 设备是手机屏，此外 Android 也支持其他的外部输入设备如 HDMI, Wifi Display 等，
   将 SurfaceFlinger, OpenGL, HWComposer 合成后的数据输出到 Display 设备的缓存区用来显示。
//todo 增加线程，cpu线程，进程,gpu等完成流程图，软硬件绘制  RootViewImpl
setContentView  将DecorView设置到phoneWindow
ActivityThread  handleResumeActivity  将DecorView设置到WindowManager
WindowManagerGlobal 将DecorView设置给RootViewImpl

view.invalidate
RootViewImpl.scheduleTraversals
mChoreographer.postCallback   监听Vsync的回调   通过DisplayEventReceiver注册Vsync
  doTraversal->performTraversals 开始测量布局绘制
----软件绘制
RootViewImpl.drawSoftware
mSurface.lockCanvas(dirty)   
     从BufferQueue取出一个GraphicBuffer进行绘制，创建SkBitmap, 并让其像素数据指向 GraphicBuffer 的内存地址
  mView.draw(canvas)  ViewGroup和子view在SkiaCanvas进行绘制
Surface.unlockCanvasAndPost  将GraphicBuffer送入BufferQueue
SurfaceFlinger 在BufferQueueConsumer取出GraphicBuffer，layer合成操作，送入HWComposer进行绘制
----硬件绘制
ThreadedRenderer.draw
  将View 的绘制操作(drawLine...)抽象成 DrawOp 操作并存入 DisplayList 中
  首先分配缓存区(同软件绘制)，然后将 Surface 绑定到 Render 线程，最后通过 GPU 渲染 DrawOp 数据
  交给SurfaceFlinger绘制
//todo 硬件加速的具体操作，怎么更新内容



https://github.com/huanzhiyazi/articles/blob/master/%E6%8A%80%E6%9C%AF/android/Android%E6%98%BE%E7%A4%BA%E7%B3%BB%E7%BB%9F%E4%B9%8B%E2%80%94%E2%80%94framebuffer%E5%92%8CVsync/Android%E6%98%BE%E7%A4%BA%E7%B3%BB%E7%BB%9F%E4%B9%8B%E2%80%94%E2%80%94%E5%A4%9A%E7%BC%93%E5%86%B2%E5%92%8CVsync.md
todo framebuffer和  page-flipping双缓冲

