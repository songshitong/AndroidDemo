

Web crawler：网络爬虫
• 网络爬虫模拟人类使用浏览器浏览、操作页面的行为，对互联网的站点进行操作
• 网络爬虫获取到一个页面后，会分析出页面里的所有 URI，沿着这些 URI 路径递归的遍历所有页面，因此被称为爬 虫（ Webcrawler ）、
   蜘蛛（ Spider ）、网络机器人（spiderbot）  类似蜘蛛，常见的搜索引擎
   
对待网络爬虫的 2 种态度：欢迎常光临
• SEO（Search Engine Optimization），搜索引擎优化
  • “合法”的优化：sitemap、title、keywords、https 等
  • “非法”的优化：利用 PageRank 算法漏洞    
    PageRank  大部分网页都指向的，认为是用户想要的。 
     非法商家，制造一个流量非常大的站点，大量的僵尸页面指向客户的页面，提升客户的页面  收费过程
     
     
  亚洲航空提供低价机票，第三方机构爬取低价信息然后售卖，正常用户买不到官方票了  12306，电商比价，新浪微博的粉丝、私聊信息    
对待网络爬虫的 2 种态度：拒绝访问     
  • 为了对抗网络爬虫而生的图形验证码     
  • 为了对抗图形验证码而生的“打码平台(captcha human bypass)”   题目是有限的，人工标注
    • 升级图形验证码  动作类的，短信类的
    
    
网络爬虫如何抓取数据？

• 模拟浏览器渲染引擎，需要对 JavaScript 文件分析执行、发起 Ajax 请求 等
• 爬虫爬取数据的速度 VS 互联网生成信息的速度
   • 爬虫执行速度快，许多爬虫可以并发执行
   • 互联网生成信息的速度远大于爬取速度
• 优先爬取更重要的页面    
      深度优先还是广度优先    
      深度优先可以复用链接，都是一个域的   入口页面，二级页面，。。。
      广度优先  不同频道，财经，体育   没有连接复用
      一般两个结合
      
     
     
爬虫常见的请求头部
• User-Agent：识别是哪类爬虫
• From：提供爬虫机器人管理者的邮箱地址
• Accept：告知服务器爬虫对哪些资源类型感兴趣
• Referer：相当于包含了当前请求的页面 URI  



早上一般网络爬虫爬取
老师的抓包  spider.pacp
  第1个请求 from googlebot   Google的爬虫    user-agent  googlebot   accept:text/html 感兴趣的资源
  
  第7个请求 user-agent  baiduspider  百度的爬虫
  第10个  user-agent  SemrushBot  http://www.semrush.com/bot.html 



robots.txt：告知爬虫哪些内容不应爬取     网络爬虫可能不遵循
• Robots exclusion protocol：http://www.robotstxt.org/orig.html
• robots.txt 文件内容
   • User-agent：允许哪些机器人
   • Disallow：禁止访问特定目录
   • Crawl-delay：访问间隔秒数   补充的字段
   • Allow：抵消 Disallow 指令  补充的字段
   • Sitemap：指出站点地图的 URI  补充的字段
   
第二个请求  dotbot这个蜘蛛就先爬取robots.txt 然后进行后续的爬虫
  Mozilla/5.0 (compatible; DotBot/1.1; http://www.opensiteexplorer.org/dotbot, help@moz.com) 
  
google的robots.txt  
https://www.google.com/robots.txt  


https://www.sohu.com/robots.txt
User-agent: Baiduspider
Disallow: /*?*     不允许百度蜘蛛的爬取  




哈哈
为什么增加验证码后就能防止网站被爬取呢
作者回复: 因为人可以通过思维模糊分析得出结论，而爬虫算法不好使。这样网站发现不正常访问时，就可以弹出验证码页面，要求人类输入正确后再访问。